Feature Request: High-Volume Batch Research System
I need a robust batch research system that can accept up to 100 company names at once, process them in parallel batches of 3-5 companies, save each result individually, handle all failure scenarios with automatic retry, detect and remove duplicates, and scale to store thousands of reports. This system must be optimized for maximum speed, accuracy, and reliability.
1. Company Input Methods (Up to 100 Companies)
Primary Input: CSV Upload
Accepted formats:
- Single column CSV with header: company_name
- Two column CSV: company_name, investor_group (optional grouping)
- Three column CSV: company_name, investor_group, priority (1-3, for processing order)

Example CSV:
company_name,investor_group,priority
"Acme Corporation",AEA,1
"Beta Industries",StonePeak,2
"Gamma Tech LLC",AEA,1

Secondary Input: Paste/Text Area

Accept plain text with one company per line:
- Comma-separated: "Company A, Company B, Company C"
- Newline-separated (preferred):
  Company A
  Company B
  Company C
- Smart parsing to handle variations:
  - "Company Name Inc." â†’ normalize
  - "  Company Name  " â†’ trim whitespace
  - "Company Name, LLC" â†’ preserve legal suffixes

Tertiary Input: JSON Upload
{
  "companies": [
    {"name": "Company A", "group": "AEA", "priority": 1},
    {"name": "Company B", "group": "StonePeak", "priority": 2}
  ],
  "settings": {
    "researchDepth": "standard",
    "skipExisting": true
  }
}

Input Validation & Pre-Processing
Before accepting batch:
1. Parse and normalize all company names
2. Remove exact duplicates from input
3. Flag potential duplicates (fuzzy match >90% similarity)
4. Validate count â‰¤ 100 companies
5. Check against existing saved reports for duplicates
6. Show preview with:
   - Total companies to research: X
   - Duplicates in input removed: Y
   - Already exists in saved reports: Z (option to skip or re-research)
   - Estimated completion time: ~N minutes

2. Parallel Batch Processing Engine
Processing Configuration
BATCH_SIZE: 3-5 companies processed simultaneously (configurable)
  - Default: 3 (conservative, higher reliability)
  - Turbo mode: 5 (faster, requires stable API)
  - Auto-adjust based on recent success rate

TIMEOUT_PER_COMPANY: 120 seconds (2 minutes max per company research)
BATCH_TIMEOUT: 300 seconds (5 minutes max per batch of 3-5)
MAX_RETRIES_PER_COMPANY: 3
RETRY_DELAY_BASE: 5 seconds (exponential backoff: 5s, 15s, 45s)
COOLDOWN_BETWEEN_BATCHES: 2 seconds (prevents rate limiting)

Queue Management System
// Job Queue Structure
{
  jobId: "uuid",
  status: "pending" | "processing" | "completed" | "failed" | "paused",
  createdAt: timestamp,
  startedAt: timestamp,
  completedAt: timestamp,
  
  // Company tracking
  totalCompanies: 100,
  pendingQueue: ["Company A", "Company B", ...],  // Not yet started
  activeQueue: ["Company C", "Company D", "Company E"],  // Currently processing
  completedQueue: [{name: "Company F", reportId: "xyz", duration: 45}],
  failedQueue: [{name: "Company G", attempts: 3, lastError: "timeout", willRetry: false}],
  retryQueue: [{name: "Company H", attempts: 1, nextRetryAt: timestamp}],
  
  // Progress metrics
  progress: {
    completed: 50,
    failed: 2,
    pending: 48,
    percentComplete: 50,
    averageTimePerCompany: 42,  // seconds
    estimatedTimeRemaining: 2016  // seconds
  },
  
  // Settings
  config: {
    batchSize: 3,
    researchDepth: "standard",
    skipExisting: true,
    investorGroup: "AEA"
  }
}

Parallel Processing Logic
ALGORITHM: Sliding Window Batch Processor

1. Initialize job with all companies in pendingQueue
2. Move BATCH_SIZE companies from pendingQueue â†’ activeQueue
3. Start parallel research for all activeQueue companies
4. As each company completes:
   a. If SUCCESS: Move to completedQueue, save report immediately
   b. If FAILURE: 
      - If attempts < MAX_RETRIES: Move to retryQueue with backoff timer
      - If attempts >= MAX_RETRIES: Move to failedQueue (permanent)
   c. Pull next company from pendingQueue â†’ activeQueue (maintain batch size)
5. Process retryQueue items when their backoff timer expires
6. Continue until pendingQueue AND retryQueue are empty
7. Finalize job: compile statistics, notify user

 Individual Report Saving
Save Immediately on Completion

Each company report saves independently:
- Do NOT wait for batch or job completion
- Save to database within 2 seconds of research completion
- Generate unique reportId immediately
- User can view partial results while job continues

Report Structure:
{
  id: "uuid",
  companyName: "Acme Corp",
  normalizedName: "acme_corp",  // For duplicate detection
  investorGroup: "AEA",
  batchJobId: "job_xyz",  // Link to parent batch job
  
  researchData: { ... },  // Full research results
  
  metadata: {
    createdAt: timestamp,
    updatedAt: timestamp,
    researchDuration: 42,  // seconds
    dataSourcesUsed: ["web", "sec_filings", "news"],
    confidenceScore: 0.87,
    version: 1
  },
  
  status: "active" | "archived" | "deleted"
}

Real-Time Progress Updates
WebSocket or polling updates every 2 seconds:
{
  jobId: "xyz",
  event: "company_completed",
  data: {
    companyName: "Acme Corp",
    reportId: "report_123",
    position: "15 of 100",
    timeElapsed: "6m 30s",
    estimatedRemaining: "35m 45s"
  }
}

Comprehensive Failure Handling
Failure Categories & Responses
Category 1: Transient Failures (Auto-Retry)
- API timeout â†’ Retry with exponential backoff
- Rate limit (429) â†’ Pause batch, wait, resume
- Network error â†’ Retry up to 3 times
- Temporary service unavailable (503) â†’ Wait 30s, retry
- Partial data received â†’ Retry for complete data

Response: Add to retryQueue with appropriate delay

Category 2: Company-Specific Failures (Skip & Log)
- Company not found / insufficient data â†’ Mark as "no_data", skip
- Ambiguous company name (multiple matches) â†’ Flag for manual review
- Invalid company name format â†’ Log error, skip
- Blocked/restricted content â†’ Log reason, skip

Response: Move to failedQueue with detailed error, continue processing

Category 3: System Failures (Pause & Alert)
- Database connection lost â†’ Pause job, retry connection, resume
- Memory exceeded â†’ Reduce batch size, continue
- API credentials invalid â†’ Pause job, alert user
- Critical error â†’ Save state, pause job, alert user

Response: Pause job, attempt recovery, alert user if unrecoverable

Category 4: Job-Level Failures (Recoverable)
- Server restart mid-job â†’ Job persists in database, auto-resume on restart
- User session timeout â†’ Job continues in background
- Browser closed â†’ Job continues, user can check status later

Response: Jobs are stateful and survive interruptions

Retry Logic Implementation
async function processWithRetry(company, attempt = 1) {
  const maxRetries = 3;
  const baseDelay = 5000; // 5 seconds
  
  try {
    const result = await researchCompany(company, {
      timeout: 120000,  // 2 minute timeout
      signal: AbortSignal.timeout(120000)
    });
    return { success: true, data: result };
    
  } catch (error) {
    const isRetryable = isTransientError(error);
    
    if (isRetryable && attempt < maxRetries) {
      const delay = baseDelay * Math.pow(3, attempt - 1); // 5s, 15s, 45s
      await sleep(delay);
      return processWithRetry(company, attempt + 1);
    }
    
    return { 
      success: false, 
      error: error.message,
      attempts: attempt,
      retryable: isRetryable && attempt < maxRetries
    };
  }
}

Failure Recovery Dashboard
Show user clear status of all failures:
- Companies that failed permanently (with reason)
- Companies pending retry (with countdown)
- Option to manually retry any failed company
- Option to skip all remaining retries
- Export failed companies list for manual research

5. Duplicate Detection & Removal
Multi-Layer Duplicate Detection
Layer 1: Input Deduplication (Before Processing)

- Exact match: Remove identical company names
- Case-insensitive: "ACME Corp" = "acme corp"
- Whitespace normalization: "Acme  Corp" = "Acme Corp"
- Legal suffix normalization: "Acme Corp Inc" â‰ˆ "Acme Corporation"
- Punctuation normalization: "Acme, Corp." = "Acme Corp"

Layer 2: Fuzzy Matching (Pre-Processing Check)
Algorithm: Levenshtein distance + Jaro-Winkler similarity

Flag potential duplicates if similarity > 85%:
- "Acme Corporation" vs "Acme Corp" â†’ 92% similar, flag
- "Beta Industries" vs "Beta Industrial" â†’ 88% similar, flag

User resolution modal:
"These companies appear similar. Keep both or merge?"
[Keep Both] [Merge to: Acme Corporation] [Skip Second]

Layer 3: Existing Report Check (Pre-Processing)
Before starting research, check saved reports:
- Exact match on normalizedName â†’ Skip or offer to update
- Similar names (>85%) â†’ Show comparison, let user decide

Options:
[ ] Skip companies that already have reports
[ ] Update existing reports with fresh research
[ ] Create new reports (allow duplicates)

Layer 4: Post-Processing Cleanup
After batch completes, scan for duplicates created during batch:
- Same company researched twice due to race condition
- Keep most recent/complete report
- Archive older duplicate

Automated cleanup job runs:
- Immediately after batch completion
- Daily at 2 AM for all user reports
- On-demand via "Clean Duplicates" button

Duplicate Resolution System
// Normalized name generation for consistent matching
function normalizeCompanyName(name) {
  return name
    .toLowerCase()
    .replace(/[.,\/#!$%\^&\*;:{}=\-_`~()]/g, '')  // Remove punctuation
    .replace(/\s+/g, '_')  // Spaces to underscores
    .replace(/(inc|llc|ltd|corp|corporation|company|co)$/g, '')  // Remove suffixes
    .trim();
}

// Duplicate check before save
async function saveReportWithDeduplication(report) {
  const normalized = normalizeCompanyName(report.companyName);
  
  const existing = await db.reports.findOne({
    normalizedName: normalized,
    userId: report.userId,
    status: 'active'
  });
  
  if (existing) {
    // Archive old, save new
    await db.reports.update(existing.id, { status: 'archived' });
    await logDuplicateResolution(existing.id, report.id, 'auto_archived');
  }
  
  return await db.reports.create({
    ...report,
    normalizedName: normalized
  });
}

Scalable Storage Architecture
Database Schema for High Volume
-- Main reports table (optimized for large datasets)
CREATE TABLE reports (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES users(id),
  company_name VARCHAR(255) NOT NULL,
  normalized_name VARCHAR(255) NOT NULL,  -- Indexed for deduplication
  investor_group VARCHAR(100),
  
  -- Research data stored as JSONB for flexibility
  research_data JSONB NOT NULL,
  
  -- Metadata
  status VARCHAR(20) DEFAULT 'active',  -- active, archived, deleted
  batch_job_id UUID REFERENCES batch_jobs(id),
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  research_duration_seconds INTEGER,
  confidence_score DECIMAL(3,2),
  version INTEGER DEFAULT 1,
  
  -- Indexes for performance
  INDEX idx_reports_user_status (user_id, status),
  INDEX idx_reports_normalized_name (normalized_name),
  INDEX idx_reports_investor_group (investor_group),
  INDEX idx_reports_created_at (created_at DESC),
  INDEX idx_reports_batch_job (batch_job_id)
);

-- Partitioning for very large datasets (>100K reports)
-- Partition by user_id or created_at month

-- Batch jobs table
CREATE TABLE batch_jobs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES users(id),
  status VARCHAR(20) NOT NULL,
  config JSONB NOT NULL,
  progress JSONB NOT NULL,
  
  -- Queue management (stored as JSONB arrays for flexibility)
  pending_queue JSONB DEFAULT '[]',
  active_queue JSONB DEFAULT '[]',
  completed_queue JSONB DEFAULT '[]',
  failed_queue JSONB DEFAULT '[]',
  retry_queue JSONB DEFAULT '[]',
  
  created_at TIMESTAMPTZ DEFAULT NOW(),
  started_at TIMESTAMPTZ,
  completed_at TIMESTAMPTZ,
  
  INDEX idx_batch_jobs_user_status (user_id, status),
  INDEX idx_batch_jobs_created_at (created_at DESC)
);

-- Archived reports (moved here after 90 days or manual archive)
CREATE TABLE reports_archive (
  -- Same schema as reports
  -- Used for cold storage, slower queries acceptable
);

Storage Capacity Planning
Target: Support 10,000+ reports per user

Report size estimation:
- Average research_data JSONB: ~50KB
- With metadata: ~55KB per report
- 10,000 reports = ~550MB per user

Optimization strategies:
1. JSONB compression (automatic in PostgreSQL)
2. Pagination (never load all reports at once)
3. Virtual scrolling in UI (render only visible rows)
4. Lazy loading of full research_data (load summary first)
5. Archive old reports (>90 days) to separate table
6. Optional: S3/Cloud storage for research_data blobs

Efficient Querying
// Paginated report fetching with cursor-based pagination
async function getReports(userId, options) {
  const {
    limit = 50,
    cursor = null,  // Last report ID from previous page
    investorGroup = null,
    status = 'active',
    sortBy = 'created_at',
    sortOrder = 'desc'
  } = options;
  
  let query = db.reports
    .where('user_id', userId)
    .where('status', status);
  
  if (investorGroup) {
    query = query.where('investor_group', investorGroup);
  }
  
  if (cursor) {
    query = query.where('id', '<', cursor);  // Cursor-based pagination
  }
  
  return query
    .orderBy(sortBy, sortOrder)
    .limit(limit)
    .select([
      'id', 'company_name', 'investor_group', 
      'created_at', 'confidence_score',
      'research_data->summary as summary'  // Only fetch summary, not full data
    ]);
}

Performance Optimization
Speed Optimizations
1. Connection Pooling

// Database connection pool
const pool = new Pool({
  max: 20,  // Maximum concurrent connections
  min: 5,   // Minimum idle connections
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

// HTTP client with keep-alive
const httpAgent = new Agent({
  keepAlive: true,
  maxSockets: 50,
  maxFreeSockets: 10,
  timeout: 60000,
});

2. Caching Layer
// Redis cache for frequently accessed data
const cache = {
  // Cache company research for 24 hours (avoid re-researching same company)
  companyResearch: { ttl: 86400 },
  
  // Cache user's report list for 5 minutes
  reportList: { ttl: 300 },
  
  // Cache batch job status for 10 seconds (real-time updates)
  jobStatus: { ttl: 10 },
};

// Check cache before research
async function researchWithCache(companyName, userId) {
  const cacheKey = `research:${normalizeCompanyName(companyName)}`;
  const cached = await redis.get(cacheKey);
  
  if (cached && !options.forceRefresh) {
    return JSON.parse(cached);
  }
  
  const result = await performResearch(companyName);
  await redis.setex(cacheKey, 86400, JSON.stringify(result));
  return result;
}

3. Parallel Processing Optimization
// Process batch with controlled concurrency
async function processBatchOptimized(companies, concurrency = 3) {
  const results = [];
  const executing = new Set();
  
  for (const company of companies) {
    const promise = processCompany(company).then(result => {
      executing.delete(promise);
      return result;
    });
    
    executing.add(promise);
    results.push(promise);
    
    if (executing.size >= concurrency) {
      await Promise.race(executing);
    }
  }
  
  return Promise.all(results);
}

4. Database Write Batching
// Batch database writes for efficiency
class WriteBuffer {
  constructor(flushInterval = 1000, maxSize = 10) {
    this.buffer = [];
    this.flushInterval = flushInterval;
    this.maxSize = maxSize;
    this.timer = setInterval(() => this.flush(), flushInterval);
  }
  
  async add(report) {
    this.buffer.push(report);
    if (this.buffer.length >= this.maxSize) {
      await this.flush();
    }
  }
  
  async flush() {
    if (this.buffer.length === 0) return;
    
    const toWrite = this.buffer.splice(0);
    await db.reports.insertMany(toWrite);  // Bulk insert
  }
}

5. Response Streaming
// Stream large report lists instead of loading all at once
app.get('/api/reports/export', async (req, res) => {
  res.setHeader('Content-Type', 'application/json');
  res.write('{"reports":[');
  
  let first = true;
  const cursor = db.reports.find({ userId: req.userId }).cursor();
  
  for await (const report of cursor) {
    if (!first) res.write(',');
    res.write(JSON.stringify(report));
    first = false;
  }
  
  res.write(']}');
  res.end();
});

Accuracy Optimizations
1. Research Quality Validation
// Validate research results before saving
function validateResearchQuality(result) {
  const checks = {
    hasCompanyName: !!result.companyName,
    hasDescription: result.description?.length > 100,
    hasFinancials: Object.keys(result.financials || {}).length > 0,
    hasIndustry: !!result.industry,
    dataFreshness: isRecent(result.lastUpdated, 30), // Within 30 days
  };
  
  const score = Object.values(checks).filter(Boolean).length / Object.keys(checks).length;
  
  return {
    isValid: score >= 0.6,  // At least 60% checks pass
    confidenceScore: score,
    failedChecks: Object.entries(checks)
      .filter(([_, passed]) => !passed)
      .map(([check]) => check)
  };
}

2. Source Verification
// Cross-reference multiple data sources
async function researchWithVerification(companyName) {
  const sources = await Promise.allSettled([
    searchWeb(companyName),
    searchSECFilings(companyName),
    searchNewsArticles(companyName),
    searchLinkedIn(companyName),
  ]);
  
  const successfulSources = sources
    .filter(s => s.status === 'fulfilled')
    .map(s => s.value);
  
  // Merge and verify data across sources
  return mergeAndVerify(successfulSources);
}

3. Company Name Resolution
// Resolve ambiguous company names
async function resolveCompanyName(inputName) {
  const candidates = await searchCompanyRegistry(inputName);
  
  if (candidates.length === 0) {
    return { resolved: false, error: 'Company not found' };
  }
  
  if (candidates.length === 1) {
    return { resolved: true, company: candidates[0] };
  }
  
  // Multiple matches - use scoring
  const scored = candidates.map(c => ({
    ...c,
    score: calculateMatchScore(inputName, c.name)
  }));
  
  const best = scored.sort((a, b) => b.score - a.score)[0];
  
  if (best.score > 0.95) {
    return { resolved: true, company: best };
  }
  
  return { 
    resolved: false, 
    error: 'Ambiguous name', 
    candidates: scored.slice(0, 5) 
  };
}

Capacity Optimizations
1. Resource Limits

const LIMITS = {
  maxConcurrentJobs: 3,        // Per user
  maxCompaniesPerJob: 100,
  maxActiveReports: 50000,     // Per user, archive older
  maxJobDuration: 3600,        // 1 hour max
  maxMemoryPerJob: 512,        // MB
  maxRetries: 3,
  rateLimitPerMinute: 30,      // API calls
};

2. Auto-Scaling Batch Size
// Dynamically adjust batch size based on system load
function calculateOptimalBatchSize(metrics) {
  const { 
    recentSuccessRate,
    averageResponseTime,
    currentMemoryUsage,
    activeJobs 
  } = metrics;
  
  let batchSize = 3;  // Default
  
  // Increase if performing well
  if (recentSuccessRate > 0.95 && averageResponseTime < 30000) {
    batchSize = 5;
  }
  
  // Decrease if struggling
  if (recentSuccessRate < 0.8 || averageResponseTime > 60000) {
    batchSize = 2;
  }
  
  // Decrease if system under load
  if (currentMemoryUsage > 0.8 || activeJobs > 10) {
    batchSize = Math.max(1, batchSize - 1);
  }
  
  return batchSize;
}

3. Memory Management
// Process large batches without memory overflow
async function processLargeBatch(companies) {
  const CHUNK_SIZE = 10;  // Process 10 at a time in memory
  
  for (let i = 0; i < companies.length; i += CHUNK_SIZE) {
    const chunk = companies.slice(i, i + CHUNK_SIZE);
    
    await processChunk(chunk);
    
    // Force garbage collection hint
    if (global.gc) global.gc();
    
    // Brief pause to prevent memory buildup
    await sleep(100);
  }
}

 User Interface Components
Batch Upload Screen

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ðŸ“¤ Batch Company Research                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Upload companies (max 100):                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  [Choose File]  or  drag & drop CSV here            â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  Supported: CSV, TXT, JSON                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  Or paste company names:                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Company A                                          â”‚   â”‚
â”‚  â”‚  Company B                                          â”‚   â”‚
â”‚  â”‚  Company C                                          â”‚   â”‚
â”‚  â”‚  ...                                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  âš™ï¸ Settings                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Investor Group: [AEA          â–¼]                   â”‚   â”‚
â”‚  â”‚  Processing Speed: â—‹ Normal (3)  â— Turbo (5)        â”‚   â”‚
â”‚  â”‚  â˜‘ Skip companies with existing reports             â”‚   â”‚
â”‚  â”‚  â˜ Force refresh existing reports                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  [Validate & Preview]                                       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Validation Preview Screen
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ… Validation Complete                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Summary:                                                   â”‚
â”‚  â€¢ Total companies uploaded: 100                            â”‚
â”‚  â€¢ Valid companies: 95                                      â”‚
â”‚  â€¢ Duplicates in upload: 3 (removed)                        â”‚
â”‚  â€¢ Already in saved reports: 7                              â”‚
â”‚  â€¢ Companies to research: 88                                â”‚
â”‚                                                             â”‚
â”‚  âš ï¸ Potential Issues (2):                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  "Acme Corp" - Similar to existing "Acme Corporation"â”‚   â”‚
â”‚  â”‚  [Keep Both] [Skip] [Replace Existing]              â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  "XYZ Industries" - Multiple matches found           â”‚   â”‚
â”‚  â”‚  [Select: XYZ Industries Inc â–¼]                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  Estimated time: ~45 minutes (88 companies Ã— ~30 sec)       â”‚
â”‚                                                             â”‚
â”‚  [Cancel]  [Edit List]  [ðŸš€ Start Research]                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Progress Dashboard
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ðŸ”¬ Batch Research in Progress                    [Pause]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  45%                   â”‚
â”‚  40 of 88 companies completed                               â”‚
â”‚                                                             â”‚
â”‚  â±ï¸ Elapsed: 18m 32s  |  Remaining: ~22m                    â”‚
â”‚  ðŸ“Š Success rate: 97.5%  |  Avg: 28s per company            â”‚
â”‚                                                             â”‚
â”‚  Currently Processing:                                      â”‚
â”‚  â”œâ”€ ðŸ”„ Alpha Industries (15s)                              â”‚
â”‚  â”œâ”€ ðŸ”„ Beta Corp (8s)                                      â”‚
â”‚  â””â”€ ðŸ”„ Gamma LLC (3s)                                      â”‚
â”‚                                                             â”‚
â”‚  Recent Completions:                                        â”‚
â”‚  â”œâ”€ âœ… Delta Tech (saved) - 32s                            â”‚
â”‚  â”œâ”€ âœ… Epsilon Inc (saved) - 28s                           â”‚
â”‚  â”œâ”€ âš ï¸ Zeta Corp (retry 1/3) - timeout                     â”‚
â”‚  â””â”€ âœ… Eta Industries (saved) - 25s                        â”‚
â”‚                                                             â”‚
â”‚  [View Completed Reports]  [View Failures]                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Completion Summary
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ… Batch Research Complete                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Results:                                                   â”‚
â”‚  â€¢ Successfully researched: 85 companies                    â”‚
â”‚  â€¢ Failed (no data): 2 companies                           â”‚
â”‚  â€¢ Failed (error): 1 company                               â”‚
â”‚  â€¢ Total time: 42 minutes                                  â”‚
â”‚                                                             â”‚
â”‚  âŒ Failed Companies:                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Obscure LLC - No data found       [Retry] [Skip]   â”‚   â”‚
â”‚  â”‚  Mystery Corp - No data found      [Retry] [Skip]   â”‚   â”‚
â”‚  â”‚  Problem Inc - API error           [Retry] [Skip]   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  [Retry All Failed]  [Export Failed List]                   â”‚
â”‚                                                             â”‚
â”‚  [View All 85 New Reports â†’]                                â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

9. API Endpoints

POST   /api/batch/upload          - Upload and validate company list
POST   /api/batch/start           - Start batch research job
GET    /api/batch/status/:jobId   - Get job progress (poll or WebSocket)
POST   /api/batch/pause/:jobId    - Pause running job
POST   /api/batch/resume/:jobId   - Resume paused job
POST   /api/batch/cancel/:jobId   - Cancel and cleanup job
POST   /api/batch/retry/:jobId    - Retry all failed companies
GET    /api/batch/history         - List user's batch job history

WebSocket /ws/batch/:jobId        - Real-time progress updates

Critical Success Metrics
Performance Targets:
- Process 100 companies in < 60 minutes
- Average research time: < 30 seconds per company
- Success rate: > 95%
- Zero data loss on failures
- Resume capability after any interruption

Reliability Targets:
- 99.9% job completion rate
- Automatic recovery from transient failures
- No duplicate reports created
- All failures logged with actionable details

Scalability Targets:
- Support 10,000+ saved reports per user
- Handle 10 concurrent batch jobs system-wide
- Maintain performance with large datasets
